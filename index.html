<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yiming Dou</title>

  <meta name="author" content="Yiming Dou">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"
    > -->
  <link rel="icon" href="figs/Cornell_icon.png">
  <!-- <link rel="icon" href="figs/UMich-icon.png"> -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Pacifico">
</head>

<script>
  function showSelected() {
    document.getElementById('selected_publications').style.display = 'block';
    document.getElementById('full_publications').style.display = 'none';
  }
  function showAllByYear() {
    document.getElementById('selected_publications').style.display = 'none';
    document.getElementById('full_publications').style.display = 'block';
  }
  function selectButton(id) {
    document.querySelectorAll('.button').forEach(btn => btn.classList.remove('selected-button'));
    document.getElementById(id).classList.add('selected-button');
  }
</script>

<body>
  <table
    style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <!-- Bio -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:0%;width:68%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yiming Dou </name>
                    <cname>Á™¶Èì±Êòé</cname>
                  </p>
                  <!-- bio -->
                  <p style="text-align:justify">
                    I'm a third-year Ph.D. student at <a href="https://tech.cornell.edu/">Cornell Tech</a>,
                    <!-- I'm a second-year PhD student at the <strong>University of Michigan</strong>, -->
                    advised by Prof. <a href="https://andrewowens.com/">Andrew Owens</a>.
                  </p>
                  <p style="text-align:justify">
                    I received my M.S.E. from the <a href="https://cse.engin.umich.edu/">University of Michigan</a> in 2025, before transferring to Cornell.

                    Prior to that, I received my B.Eng. and B.Ec. from <a href="https://www.cs.sjtu.edu.cn/en/">Shanghai Jiao Tong University</a> in
                    2023, with an honors degree from <a href="https://en.zhiyuan.sjtu.edu.cn/">Zhiyuan College</a>.

                    During my undergraduate, I was fortunate to work with
                    Prof. <a href="https://ruohangao.github.io/">Ruohan Gao</a>
                    and Prof. <a href="https://jiajunwu.com/">Jiajun Wu</a> at <a href="https://www.cs.stanford.edu/">Stanford</a>.
                    I also worked closely with Prof. <a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a>
                    and Prof. <a href="https://www.mvig.org/">Cewu Lu</a> at SJTU.
                  </p>
                  <p style="text-align:justify">
                    My research interests mainly lie in <strong>multimodal perception</strong>,
                    <strong>reasoning</strong>
                    and <strong>robot learning</strong>.
                  </p>

                  <!-- links -->
                  <p style="text-align:center">
                    <a href="mailto:ymdou@umich.edu">Email</a> &nbsp¬∑&nbsp
                    <!-- <a href="files/CV_Yiming Dou.pdf">CV</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=IMsjP1sAAAAJ">Google Scholar</a> &nbsp¬∑&nbsp
                    <a href="https://github.com/Dou-Yiming">Github</a> &nbsp¬∑&nbsp
                    <a href="https://twitter.com/_YimingDou">Twitter</a> &nbsp¬∑&nbsp
                    <a href="./figs/wechat.jpg">WeChat</a>
                  </p>
                </td>
                <!-- profile image -->
                <td style="padding:2%;width:32%;max-width:32%">
                  <a href="figs/profile_new_new_new.jpg"><img
                      style="width:100%;max-width:100%;border-radius:50%;object-fit:cover;aspect-ratio:1/1"
                      alt="profile photo" src="figs/profile_new_new_new.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- News -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="padding:0%;width:96%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 10px;width:100%;vertical-align:middle">
                  <p>
                    <li style="line-height:125%">
                      08/2025:
                      &#129395;
                      Our paper "Cross-Sensor Touch Generation" is selected as an oral presentation at CoRL 2025!
                    </li>
                    <li style="line-height:125%">
                      08/2025:
                      &#x1F389;
                      One paper accepted to CoRL 2025! See you in Seoul!
                    </li>
                    <li style="line-height:125%">
                      02/2025:
                      &#x1F389;
                      One paper accepted to CVPR 2025! See you in Nashville!
                    </li>
                    <li style="line-height:125%">
                      01/2025:
                      &#x1F389;
                      Two papers accepted to ICRA 2025! See you in Atlanta!
                    </li>
                    <li style="line-height:125%">
                      09/2024:
                      &#129395;
                      Honored to be selected as Outstanding Reviewer for ECCV 2024!
                    </li>
                    <li style="line-height:125%">
                      02/2024:
                      &#x1F389;
                      Three papers accepted to CVPR 2024! See you in Seattle!
                    </li>
                    <li style="line-height:125%">
                      06/2023:
                      &#129395;
                      Graduated from SJTU with honors degree!
                    </li>
                    <!-- <li style="line-height:125%">
                      04/2023:
                      &#128640;
                      Selected as SJTU Outstanding Graduate.
                    </li> -->
                    <!-- <li style="line-height:125%">
                      04/2023:
                      &#129395;
                      I will join Prof. Andrew Owens's group at the University of Michigan as a PhD student this fall.
                      Looking forward to the wonderful life in Ann Arbor!
                    </li> -->
                    <li style="line-height:125%">
                      02/2023:
                      &#x1F389;
                      One paper accepted to CVPR 2023!
                    </li>
                  </p>
                </td>
              </tr>

            </tbody>
          </table>
          <!-- Research Interests -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px;width:100%;vertical-align:middle">
                  <heading>Research Interests</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="padding:0%;width:96%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 10px;width:100%;vertical-align:middle;text-align:justify"">
                  <p style="text-align:justify;margin-bottom:5px">
                  Humans perceive the world with multiple senses,
                  based on which we establish abstract concepts to understand it.
                  From the concepts we develop logical reasoning ability,
                  and thus creating brilliant achievements.
                  Inspired by this,
                  my dream is to design human-like multisensory intelligent systems,
                  which can be divided into four specific problems:
                  <li style="line-height:125%">
                    <strong>Multimodal Perception</strong>:
                    how to perceive and model
                    the multimodal physical world.
                  </li>
                  <li style="line-height:125%">
                    <strong>Concept Learning</strong>:
                    how to abstract the perceived information into high-level concepts.
                  </li>
                  <li style="line-height:125%">
                    <strong>Reasoning</strong>:
                    how to perform causal reasoning on the basis of concepts.
                  </li>
                  <li style="line-height:125%">
                    <strong>Robot Learning</strong>:
                    how to enable robots to actively interact with the real-world environments and humans.
                  </li>
                  </p>
                </td>
              </tr>

            </tbody>
          </table>
          <!-- Publications -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading> (
                  <button class="button selected-button" id="btn-selected"
                    onclick="showSelected(); selectButton('btn-selected')">
                    Show selected
                  </button>
                  /
                  <button class="button" id="btn-all" onclick="showAllByYear(); selectButton('btn-all')">
                    Show all by date
                  </button>
                  )
                  <p style="margin-bottom:5px;margin-top:5px">(* indicates equal contribution)</p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- 2.2 Selected (default) -->
          <div id="selected_publications">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <video src="figs/publications/t2d2.mp4" width="250" autoplay muted playsinline loop></video>
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Cross-Sensor Touch Generation</papertitle>
                    <!-- authors -->
                    <br>
                    <a
                      href="https://scholar.google.com/citations?hl=en&user=40qJCVcAAAAJ&view_op=list_works&gmla=AOAOcb2_9BvSpCB7V9TJdqUXyEh-cyIvxbucDLBeQSWXW_zc59fx9wanTJUXb_0qTnmXky4zE9tjvOWTBLjEklmo">Samanta
                      Rodriguez</a>*,
                    <strong>Yiming Dou</strong>*,
                    <a href="https://mr-mikmik.github.io/">Miquel Oller</a>,
                    <a href="https://andrewowens.com/">Andrew Owens</a>,
                    <a href="https://www.mmintlab.com/people/nima-fazeli/">Nima Fazeli</a>
                    <!-- conference & date -->
                    <br>
                    CoRL 2025 (<strong style="color: rgb(190, 11, 11);">Oral</strong>)
                    <br>
                    <!-- links -->
                    <a href="https://openreview.net/pdf?id=oGcC8nMOit">paper</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We learn to translate touch signals captured from one touch sensor to another, which allows us to transfer object manipulation policies between sensors. 
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <video src="figs/publications/hearing_hands.mp4" width="250" autoplay muted playsinline
                      loop></video>
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes
                    </papertitle>
                    <!-- authors -->
                    <br>
                    <strong>Yiming Dou</strong>,
                    <a href="https://prbs5kong.github.io/">Wonseok Oh</a>,
                    <a href="https://www.linkedin.com/in/yuqing-luo-452715249/">Yuqing Luo</a>,
                    <a href="https://antonilo.github.io/">Antonio Loquercio</a>,
                    <a href="https://andrewowens.com/">Andrew Owens</a>
                    <!-- conference & date -->
                    <br>
                    CVPR 2025
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2506.09989">paper</a>
                    ¬∑ <a href="https://www.yimingdou.com/hearing_hands/">project page</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We make 3D scene reconstruction interactive by predicting the sounds of human hands physically interacting with the scene.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;max-width:25%;vertical-align:top" align="center">
                    <video src="figs/publications/tarf.mp4" width="250"  autoplay muted playsinline loop></video>
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Tactile-Augmented Radiance Fields
                    </papertitle>
                    <!-- authors -->
                    <br>
                    <strong>Yiming Dou</strong>,
                    <a href="https://fredfyyang.github.io/">Fengyu Yang</a>,
                    <a href="">Yi Liu</a>,
                    <a href="https://antonilo.github.io/">Antonio Loquercio</a>,
                    <a href="https://andrewowens.com/">Andrew Owens</a>
                    <!-- conference & date -->
                    <br>
                    CVPR 2024
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2405.04534">paper</a>
                    ¬∑ <a href="https://dou-yiming.github.io/TaRF/">project page</a>
                    ¬∑ <a href="https://github.com/Dou-Yiming/TaRF">code</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We present a visuo-tactile 3D scene representation that can estimate the visual and tactile signals for a given 3D position within the scene.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <video src="figs/publications/objectfolder_benchmark.mp4" width="250" autoplay muted playsinline
                      loop></video>
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>The ObjectFolder Benchmark: Multisensory Learning with Neural and Real
                      Objects</papertitle>
                    <!-- authors -->
                    <br>
                    <a href="https://ai.stanford.edu/~rhgao/">Ruohan Gao</a>*,
                    <strong>Yiming Dou</strong>*,
                    <a href="https://haolirobo.github.io/">Hao Li</a>*,
                    <a href="https://tanmay-agarwal.com/">Tanmay Agarwal</a>,
                    <a href="http://web.stanford.edu/~bohg/">Jeannette Bohg</a>,
                    <a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
                    <a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>,
                    <a href="https://jiajunwu.com/">Jiajun Wu</a>
                    <!-- conference & date -->
                    <br>
                    CVPR 2023
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2306.00956">paper</a>
                    ¬∑ <a href="https://objectfolder.stanford.edu/">project page</a>
                    ¬∑ <a href="https://github.com/objectfolder">code</a>
                    ¬∑ <a href="https://www.objectfolder.org/swan_vis/">interactive demo</a>
                    ¬∑ <a href="https://www.youtube.com/watch?v=VhXDempUYgE">video</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We introduce a benchmark suite for multisensory object-centric learning with sight, sound, and touch. 
                      We also introduce a dataset including the multisensory measurements for real-world objects
                    </p>
                  </td>
                </tr>

              </tbody>
            </table>
          </div>

          <!-- 2.3 Full list, grouped by year (hidden initially) -->
          <div id="full_publications" style="display: none;">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <video src="figs/publications/t2d2.mp4" width="250" autoplay muted playsinline loop></video>
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Cross-Sensor Touch Generation</papertitle>
                    <!-- authors -->
                    <br>
                    <a
                      href="https://scholar.google.com/citations?hl=en&user=40qJCVcAAAAJ&view_op=list_works&gmla=AOAOcb2_9BvSpCB7V9TJdqUXyEh-cyIvxbucDLBeQSWXW_zc59fx9wanTJUXb_0qTnmXky4zE9tjvOWTBLjEklmo">Samanta
                      Rodriguez</a>*,
                    <strong>Yiming Dou</strong>*,
                    <a href="https://mr-mikmik.github.io/">Miquel Oller</a>,
                    <a href="https://andrewowens.com/">Andrew Owens</a>,
                    <a href="https://www.mmintlab.com/people/nima-fazeli/">Nima Fazeli</a>
                    <!-- conference & date -->
                    <br>
                    CoRL 2025 (<strong style="color: rgb(190, 11, 11);">Oral</strong>)
                    <br>
                    <!-- links -->
                    <a href="https://openreview.net/pdf?id=oGcC8nMOit">paper</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We learn to translate touch signals captured from one touch sensor to another, which allows us to transfer object manipulation policies between sensors. 
                    </p>
                  </td>
                </tr>

                <tr>
                  <!-- <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <img src="figs/publications/hearing_hands.jpg" style="border-style: none" width="250">
                  </td> -->
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <video src="figs/publications/hearing_hands.mp4" width="250" autoplay muted playsinline
                      loop></video>
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes
                    </papertitle>
                    <!-- authors -->
                    <br>
                    <strong>Yiming Dou</strong>,
                    <a href="https://prbs5kong.github.io/">Wonseok Oh</a>,
                    <a href="https://www.linkedin.com/in/yuqing-luo-452715249/">Yuqing Luo</a>,
                    <a href="https://antonilo.github.io/">Antonio Loquercio</a>,
                    <a href="https://andrewowens.com/">Andrew Owens</a>
                    <!-- conference & date -->
                    <br>
                    CVPR 2025
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2506.09989">paper</a>
                    ¬∑ <a href="https://www.yimingdou.com/hearing_hands/">project page</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We make 3D scene reconstruction interactive by predicting the sounds of human hands physically interacting with the scene.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <img src="figs/publications/cttp.jpg" style="border-style: none" width="250">
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Contrastive Touch-to-Touch Pretraining
                    </papertitle>
                    <!-- authors -->
                    <br>
                    <a
                      href="https://scholar.google.com/citations?hl=en&user=40qJCVcAAAAJ&view_op=list_works&gmla=AOAOcb2_9BvSpCB7V9TJdqUXyEh-cyIvxbucDLBeQSWXW_zc59fx9wanTJUXb_0qTnmXky4zE9tjvOWTBLjEklmo">Samanta
                      Rodriguez</a>*,
                    <strong>Yiming Dou</strong>*,
                    <a href="https://scholar.google.com/citations?user=pyUE2OIAAAAJ&hl=en&authuser=1&oi=ao">William van
                      den Bogert</a>,
                    <a href="https://mr-mikmik.github.io/">Miquel Oller</a>,
                    <a href="">Kevin So</a>,
                    <a href="https://andrewowens.com/">Andrew Owens</a>,
                    <a href="https://www.mmintlab.com/people/nima-fazeli/">Nima Fazeli</a>
                    <!-- conference & date -->
                    <br>
                    ICRA 2025
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2410.11834">paper</a>
                    ¬∑ <a href="https://www.mmintlab.com/research/cttp/">project page</a>
                    ¬∑ <a href="https://github.com/MMintLab/CTTP">code</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We learn a unified representation that captures the shared information between different tactile sensors using contrastive learning.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <img src="figs/publications/tactile_functa.png" style="border-style: none" width="250">
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Tactile Functasets: Neural Implicit Representations of Tactile Datasets
                    </papertitle>
                    <!-- authors -->
                    <br>
                    <a href="https://sikai-li.gitbook.io/sikai_li">Sikai Li</a>,
                    <a
                      href="https://scholar.google.com/citations?hl=en&user=40qJCVcAAAAJ&view_op=list_works&gmla=AOAOcb2_9BvSpCB7V9TJdqUXyEh-cyIvxbucDLBeQSWXW_zc59fx9wanTJUXb_0qTnmXky4zE9tjvOWTBLjEklmo">Samanta
                      Rodriguez</a>,
                    <strong>Yiming Dou</strong>,
                    <a href="https://andrewowens.com/">Andrew Owens</a>,
                    <a href="https://www.mmintlab.com/people/nima-fazeli/">Nima Fazeli</a>
                    <!-- conference & date -->
                    <br>
                    ICRA 2025
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2409.14592">paper</a>
                    ¬∑ <a href="https://www.mmintlab.com/tactile-functasets/">project page</a>
                    ¬∑ <a href="https://github.com/MMintLab/tactile_functasets">code</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We introduce a novel implicit function representation for tactile signals, which improves the efficiency and generalization of tactile signals.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <video src="figs/publications/touch2touch.mp4" width="250" autoplay muted playsinline loop></video>
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation</papertitle>
                    <!-- authors -->
                    <br>
                    <a
                      href="https://scholar.google.com/citations?hl=en&user=40qJCVcAAAAJ&view_op=list_works&gmla=AOAOcb2_9BvSpCB7V9TJdqUXyEh-cyIvxbucDLBeQSWXW_zc59fx9wanTJUXb_0qTnmXky4zE9tjvOWTBLjEklmo">Samanta
                      Rodriguez</a>*,
                    <strong>Yiming Dou</strong>*,
                    <a href="https://mr-mikmik.github.io/">Miquel Oller</a>,
                    <a href="https://andrewowens.com/">Andrew Owens</a>,
                    <a href="https://www.mmintlab.com/people/nima-fazeli/">Nima Fazeli</a>
                    <!-- conference & date -->
                    <br>
                    arXiv 2024
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2409.08269">paper</a>
                    ¬∑ <a href="https://www.mmintlab.com/research/touch2touch/">project page</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We learn to translate touch signals captured from one touch sensor to another, which allows us to transfer object manipulation policies between sensors. 
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <video src="figs/publications/tarf.mp4" width="250" autoplay muted playsinline loop></video>
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Tactile-Augmented Radiance Fields
                    </papertitle>
                    <!-- authors -->
                    <br>
                    <strong>Yiming Dou</strong>,
                    <a href="https://fredfyyang.github.io/">Fengyu Yang</a>,
                    <a href="">Yi Liu</a>,
                    <a href="https://antonilo.github.io/">Antonio Loquercio</a>,
                    <a href="https://andrewowens.com/">Andrew Owens</a>
                    <!-- conference & date -->
                    <br>
                    CVPR 2024
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2405.04534">paper</a>
                    ¬∑ <a href="https://dou-yiming.github.io/TaRF/">project page</a>
                    ¬∑ <a href="https://github.com/Dou-Yiming/TaRF">code</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We present a visuo-tactile 3D scene representation that can estimate the visual and tactile signals for a given 3D position within the scene.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <img src="figs/publications/unitouch.png" style="border-style: none" width="250">
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Binding Touch to Everything: Learning Unified Multimodal Tactile Representations
                    </papertitle>
                    <!-- authors -->
                    <br>
                    <a href="https://fredfyyang.github.io/">Fengyu Yang</a>*,
                    <a href="https://cfeng16.github.io/">Chao Feng</a>*,
                    <a href="https://ificl.github.io/">Ziyang Chen</a>*,
                    <a href="https://scholar.google.com/citations?user=A3c4pHkAAAAJ&hl=ko ">Hyoungseob Park</a>,
                    <a href="">Daniel Wang</a>,
                    <strong>Yiming Dou</strong>,
                    <a href="https://adonis-galaxy.github.io/homepage/">Ziyao Zeng</a>,
                    <a href="">Xien Chen</a>,
                    <a href="https://www.linkedin.com/in/suchisrit/">Rit Gangopadhyay</a>,
                    <a href="https://andrewowens.com/">Andrew Owens</a>,
                    <a href="https://vision.cs.yale.edu/members/alex-wong.html">Alex Wong</a>
                    <!-- conference & date -->
                    <br>
                    CVPR 2024
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2401.18084">paper</a>
                    ¬∑ <a href="https://cfeng16.github.io/UniTouch/">project page</a>
                    ¬∑ <a href="https://github.com/cfeng16/UniTouch">code</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We introduce UniTouch, a unified tactile representation for vision-based touch sensors connected to multiple modalities, including vision, language, and sound.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <img src="figs/publications/pangea.png" style="border-style: none" width="250">
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Bridging The Isolated Islands in Human Action Understanding</papertitle>
                    <!-- authors -->
                    <br>
                    <a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a>*,
                    <a href="https://scholar.google.com/citations?hl=en&user=-PHR96oAAAAJ">Xiaoqian Wu</a>*,
                    <a href="https://scholar.google.com/citations?user=DBE-ju8AAAAJ&hl=en">Xinpeng Liu</a>,
                    <strong>Yiming Dou</strong>,
                    <a href="">Yikun Ji</a>,
                    <a href="https://www.junyi42.com/">Junyi Zhang</a>,
                    <a href="">Yixing Li</a>,
                    <a href="">Jingru Tan</a>,
                    <a href="https://lucky-lance.github.io/">Xudong Lu</a>,
                    <a href="https://www.mvig.org/">Cewu Lu</a>
                    <!-- conference & date -->
                    <br>
                    CVPR 2024 (<strong style="color: rgb(190, 11, 11);">Highlight</strong>)
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2304.00553">paper</a>
                    ¬∑ <a href="https://mvig-rhos.com/pangea">project page</a>
                    ¬∑ <a href="https://github.com/DirtyHarryLYL/Sandwich">code</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We design a structured action semantic space given verb taxonomy hierarchy and align the classes of previous datasets to it.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <video src="figs/publications/objectfolder_benchmark.mp4" width="250" autoplay muted playsinline
                      loop></video>
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>The ObjectFolder Benchmark: Multisensory Learning with Neural and Real
                      Objects</papertitle>
                    <!-- authors -->
                    <br>
                    <a href="https://ai.stanford.edu/~rhgao/">Ruohan Gao</a>*,
                    <strong>Yiming Dou</strong>*,
                    <a href="https://haolirobo.github.io/">Hao Li</a>*,
                    <a href="https://tanmay-agarwal.com/">Tanmay Agarwal</a>,
                    <a href="http://web.stanford.edu/~bohg/">Jeannette Bohg</a>,
                    <a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
                    <a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>,
                    <a href="https://jiajunwu.com/">Jiajun Wu</a>
                    <!-- conference & date -->
                    <br>
                    CVPR 2023
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2306.00956">paper</a>
                    ¬∑ <a href="https://objectfolder.stanford.edu/">project page</a>
                    ¬∑ <a href="https://github.com/objectfolder">code</a>
                    ¬∑ <a href="https://www.objectfolder.org/swan_vis/">interactive demo</a>
                    ¬∑ <a href="https://www.youtube.com/watch?v=VhXDempUYgE">video</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We introduce a benchmark suite for multisensory object-centric learning with sight, sound, and touch. 
                      We also introduce a dataset including the multisensory measurements for real-world objects
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:10px 20px;width:30%;vertical-align:middle" align="center">
                    <img src="figs/publications/dio.png" style="border-style: none" width="250">
                  </td>
                  <td width="70%" valign="middle">
                    <!-- heading -->
                    <papertitle>Discovering A Variety of Objects in Spatio-Temporal Human-Object Interactions
                    </papertitle>
                    <!-- authors -->
                    <br>
                    <a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a>*,
                    <a href="https://hwfan.io/">Hongwei Fan</a>*,
                    <a href="">Zuoyu Qiu</a>,
                    <strong>Yiming Dou</strong>,
                    <a href="">Liang Xu</a>,
                    <a href="">Hao-Shu Fang</a>,
                    <a href="">Peiyang Guo</a>,
                    <a href="">Haisheng Su</a>,
                    <a href="">Dongliang Wang</a>,
                    <a href="">Wei Wu</a>,
                    <a href="https://www.mvig.org/">Cewu Lu</a>

                    <!-- conference & date -->
                    <br>
                    arXiv 2022
                    <br>
                    <!-- links -->
                    <a href="https://arxiv.org/abs/2211.07501">paper</a>
                    ¬∑ <a href="https://github.com/DirtyHarryLYL/HAKE-AVA">code</a>
                    <br>
                    <p style="margin-bottom:0px;margin-top:5px">
                      We introduce a new benchmark based on AVA: Discovering Interacted Objects (DIO) including 51 interactions and 1,000+ objects.
                    </p>
                  </td>
                </tr>

              </tbody>
            </table>
          </div>


          <!-- Teaching -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px;width:100%;vertical-align:middle">
                  <heading>Teaching</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="padding:0%;width:96%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 10px;width:100%;vertical-align:middle">
                  <p>
                    <li style="line-height:125%">
                      <strong>Graduate Student Instructor (GSI)</strong>,
                      <a href="https://www.eecs.umich.edu/courses/eecs442-ahowens/fa23/">EECS 442: Computer Vision</a>,
                      Fall 2023
                    </li>
                    </li>
                  </p>
                </td>
              </tr>

            </tbody>
          </table>
          <!-- Service -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px;width:100%;vertical-align:middle">
                  <heading>Service</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="padding:0%;width:96%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 10px;width:100%;vertical-align:middle">
                  <p>
                    <li style="line-height:125%">
                      <strong>Conference reviewer</strong>: ICRA(2025), ICLR(2025,2026), AAAI(2025), ECCV(2024), CVPR(2023,2024,2025),
                      ICCV(2023,2025), ACMMM(2025)
                    </li>
                    <li style="line-height:125%">
                      <strong>Journal reviewer</strong>: IEEE RA-L(2025)
                    </li>
                  </p>
                </td>
              </tr>

            </tbody>
          </table>
          <!-- Experience -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px;width:100%;vertical-align:middle">
                  <heading>Experience</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px 50px;width:25%;vertical-align:middle" align="center">
                  <img src="figs/Cornell_logo.png" style="border-style: none" height="100">
                </td>

                <td width="75%" valign="middle">
                  <university>Cornell Tech</university>
                  <br>
                  2025.08 ~ Present
                  <br>
                  New York, U.S.
                  <br>
                  Ph.D. Student in Computer Science
                  <br>
                  Advisor: Prof. <a href="https://andrewowens.com/">Andrew Owens</a>
                </td>
              </tr>

              <tr>
                <td style="padding:10px 50px;width:25%;vertical-align:middle" align="center">
                  <img src="figs/UMich_logo.png" style="border-style: none" height="100">
                </td>

                <td width="75%" valign="middle">
                  <university>University of Michigan</university>
                  <br>
                  2023.08 ~ 2025.08
                  <br>
                  Ann Arbor, U.S.
                  <br>
                  M.S.E. in Computer Science and Engineering
                  <br>
                  Advisor: Prof. <a href="https://andrewowens.com/">Andrew Owens</a>
                  <br>
                  Completed first two years of Ph.D. before transferring to Cornell
                  
                </td>
              </tr>

              <tr>
                <td style="padding:10px 50px;width:25%;vertical-align:middle" align="center">
                  <img src="figs/Stanford_logo.png" style="border-style: none" height="100">
                </td>

                <td width="75%" valign="middle">
                  <university>Stanford University</university>
                  <br>
                  2022.03 ~ 2023.04
                  <br>
                  Stanford, U.S.
                  <br>
                  Visiting Research Intern
                  <br>
                  Supervisor: Prof. <a href="https://ruohangao.github.io/">Ruohan Gao</a>,
                  Prof. <a href="https://jiajunwu.com/">Jiajun Wu</a>
                  and Prof. <a href="https://profiles.stanford.edu/fei-fei-li">Fei-Fei Li</a>
                </td>
              </tr>

              <tr>
                <td style="padding:10px 50px;width:25%;vertical-align:middle" align="center">
                  <img src="figs/SJTU_logo.png" style="border-style: none" height="100">
                </td>

                <td width="75%" valign="middle">
                  <!-- heading -->
                  <university>Shanghai Jiao Tong University</university>
                  <br>
                  2019.09 ~ 2023.06
                  <br>
                  Shanghai, China
                  <br>
                  B.Eng. (Honors) in Computer Science and Technology
                  <!-- (<span style="font-family: 'Pacifico', cursive;">Summa Cum Laude</span>) -->
                  <br>
                  B.Ec. (Minor) in Economics
                  <br>
                  Member of <a href="https://en.zhiyuan.sjtu.edu.cn/">Zhiyuan Honors Program</a>
                  <br>
                  Supervisor: Prof. <a href="https://www.mvig.org/">Cewu Lu</a> and Prof. <a
                    href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a>
                  <!-- conference & date -->
                </td>
              </tr>

            </tbody>
          </table>
          <!-- Honors -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px;width:100%;vertical-align:middle">
                  <heading>Selected Honors</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="padding:0%;width:96%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 10px;width:100%;vertical-align:middle">
                  <p>
                    <li style="line-height:125%">
                      <honor>Outstanding Reviewer</honor>, ECCV 2024
                    </li>
                    <li style="line-height:125%">
                      <honor>Zhiyuan Scholarship</honor> (top 30 students), SJTU, 2023
                    </li>
                    <li style="line-height:125%">
                      <honor>Outstanding Graduate</honor>, SJTU, 2023
                    </li>
                    <!-- <li style="line-height:125%">
                      <honor>Academic Excellence Scholarship</honor> (top 10%), SJTU, 2022
                    </li> -->
                    <!-- <li style="line-height:125%">
                      <honor>Zhanjiajun Scholarship</honor> (six winners at SJTU), SJTU, 2022
                    </li> -->
                    <!-- <li style="line-height:125%">
                      <honor>Meritorious Winner</honor> (top 7%), MCM, 2022
                    </li> -->
                    <!-- <li style="line-height:125%">
                      <honor>Merit Student Award</honor> (top 5%), SJTU, 2021
                    </li> -->
                    <li style="line-height:125%">
                      <honor>Zhiyuan Honors Scholarship
                      </honor> (top 5%), SJTU, 2019-2022
                    </li>
                  </p>
                </td>
              </tr>

            </tbody>
          </table>
          <!-- Misc -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px;width:100%;vertical-align:middle">
                  <heading>Misc.</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="padding:0%;width:96%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 10px;width:100%;vertical-align:middle;text-align:justify"">
                  <p style=" text-align:justify;margin-bottom:5px">
                  As a person working on building multisensory systems,
                  I also enjoy being a multisensory embodied agent outside of work:
                  <li style="line-height:125%">
                    &#128065; <strong>Photography</strong>:
                    I've been learning to take photos since I was <a href="./figs/misc/camera.jpg">7 years old</a>,
                    and have been fortunate to capture some impressive moments along the way.
                    See some of them <a href="https://www.yimingdou.com/gallery/">here</a>!
                  </li>
                  <li style="line-height:125%">
                    &#128066; <strong>Classical music</strong>:
                    I love listening to classical music, especially those from
                    the Viennese Classic period to the Romantic period.
                    <button class="button-normal-size" id="btn-music"
                      onclick="document.getElementById('favorite-music').style.display = document.getElementById('favorite-music').style.display === 'none' ? 'block' : 'none'">
                      Some of my personal favorites</button> (alphabetical order)
                    <div id="favorite-music" style="display:none; margin-left: 20px; margin-top: 1px;">
                      <ul style="margin: 0; padding-left: 0px;">
                        <li>Beethoven: Symphony No. 7 in A Major, Op. 92</li>
                        <li>Beethoven: Symphony No. 9 in D Minor, Op. 125 "Choral"</li>
                        <li>Brahms: Symphony No. 1 in C Minor, Op. 68</li>
                        <li>Mahler: Symphony No. 1 in D Major "Titan"</li>
                        <li>Mendelssohn: Piano Concerto No. 1 in G Minor, Op. 25, MWV O 7</li>
                        <li>Saint-Sa√´ns: Piano Concerto No. 2 in G Minor, Op. 22</li>
                      </ul>
                    </div>
                  </li>
                  <li style="line-height:125%">
                    &#128170; <strong>Tennis</strong>:
                    Despite having been playing for 2+ years,
                    I still regard myself as a beginner -- probably around NTRP level 3.0? --  
                    but I really enjoy it and look forward to getting better!
                  </li>
                  </p>
                </td>
              </tr>

            </tbody>
          </table>
        </td>
      </tr>
      <tr style="padding:300px" hidden>
        <script type="text/javascript" id="clustrmaps"
          src="//clustrmaps.com/map_v2.js?d=9RqQ_Wyrwtnc77JFqXfEZmLqhMIuy42lE14nZdXa6Sk&cl=ffffff&w=a"></script>
      </tr>
  </table>
  <p style="text-align:right">
    Template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>
    <br>
    Last updated on Aug 11, 2025
  </p>
</body>

</html>